<html>
  <head>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs/dist/tf.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-vis"></script>
    <script src="https://cdn.jsdelivr.net/npm/lodash@4.17.15/lodash.min.js"></script>
  </head>
  <body>
    <script>
      const MODEL_LOAD_PATH = "localstorage://model";
      let p;

      const visualizeData = (values, series, { title, xLabel, yLabel }) => {
        tfvis.render.scatterplot(
          { name: title },
          { values, series },
          {
            xLabel,
            yLabel
          }
        );
      };

      const visualizePointsAndPredictions = (
        model,
        {
          points,
          featureMinValue,
          featureMaxValue,
          labelMinValue,
          labelMaxValue,
          xLabel,
          yLabel
        }
      ) => {
        const prediction = predictionPoints(model, {
          featureMinValue,
          featureMaxValue,
          labelMinValue,
          labelMaxValue
        });
        visualizeData([points, prediction], [xLabel, "Prediction"], {
          title: `${xLabel} vs ${yLabel}`,
          xLabel,
          yLabel
        });
      };

      const visualizePointsAndClasses = async (
        model,
        { data, xLabel, yLabel }
      ) => {
        const pointsGrouped = _.groupBy(data, "class");

        visualizeData(
          Object.values(pointsGrouped),
          Object.keys(pointsGrouped),
          {
            title: `${xLabel} vs ${yLabel}`,
            xLabel,
            yLabel
          }
        );
      };

      const predictionPoints = (
        model,
        { featureMinValue, featureMaxValue, labelMinValue, labelMaxValue }
      ) => {
        const normalizedXs = tf.linspace(0, 1, 100);
        const normalizedYs = model.predict(normalizedXs.reshape([100, 1]));

        const xs = desnormalize(
          normalizedXs,
          featureMinValue,
          featureMaxValue
        ).dataSync();
        const ys = desnormalize(
          normalizedYs,
          labelMinValue,
          labelMaxValue
        ).dataSync();

        return Array.from(xs).map((val, i) => ({
          x: val,
          y: ys[i]
        }));
      };

      const loadDataset = async (xLabel, yLabel, classField) => {
        const response = await fetch(
          "http://127.0.0.1:8080/kc_house_data.json"
        );
        const json = await response.json();
        const data = tf.data.array(json);

        return data.map(d => ({
          x: d[xLabel],
          y: d[yLabel],
          class: d[classField]
        }));
      };

      const normalize = (tensor, pMin = null, pMax = null) => {
        const dimensions = tensor.shape.length > 1 && tensor.shape[1];

        if (dimensions && dimensions > 1) {
          const featuresTensor = tf.split(tensor, dimensions, 1);

          const normalizedFeatures = featuresTensor.map((featureTensor, i) => {
            return normalize(
              featureTensor,
              pMin ? pMin[i] : null,
              pMax ? pMax[i] : null
            );
          });

          const returnTensor = tf.concat(
            normalizedFeatures.map(f => f.tensor),
            1
          );

          return {
            tensor: tf.concat(
              normalizedFeatures.map(f => f.tensor),
              1
            ),
            min: normalizedFeatures.map(f => f.min),
            max: normalizedFeatures.map(f => f.max)
          };
        }

        const max = pMax || tensor.max();
        const min = pMin || tensor.min();

        // value - min / max - min
        return {
          tensor: tensor.sub(min).div(max.sub(min)),
          min,
          max
        };
      };

      const desnormalize = (tensor, min, max) => {
        const dimensions = tensor.shape.length > 1 && tensor.shape[1];

        if (dimensions && dimensions > 1) {
          const featuresTensor = tf.split(tensor, dimensions, 1);

          const desnormalizedFeatures = featuresTensor.map((featureTensor, i) =>
            desnormalize(featureTensor, min[i], max[i])
          );

          return tf.concat(desnormalizedFeatures, 1);
        }

        // ((max - min) * value) + min
        return tensor.mul(max.sub(min)).add(min);
      };

      const createModelForLinearRegression = () => {
        const model = tf.sequential();

        model.add(
          tf.layers.dense({ units: 1, inputDim: 1, activation: "linear" })
        );

        const optimizer = tf.train.sgd(0.1);
        // const optimizer = tf.train.adam();
        model.compile({
          loss: "meanSquaredError",
          optimizer
        });

        return model;
      };

      const createModelForNonLinearPrediction = () => {
        const model = tf.sequential();
        model.add(
          tf.layers.dense({
            units: 10,
            useBias: true,
            activation: "sigmoid",
            inputDim: 1
          })
        );
        model.add(
          tf.layers.dense({
            units: 10,
            useBias: true,
            activation: "sigmoid"
          })
        );
        model.add(
          tf.layers.dense({
            units: 1,
            useBias: true,
            activation: "sigmoid"
          })
        );

        model.compile({
          loss: "meanSquaredError",
          optimizer: tf.train.adam()
        });
        return model;
      };

      const createModelForBinaryClassification = () => {
        const model = tf.sequential();
        model.add(
          tf.layers.dense({
            units: 10,
            useBias: true,
            activation: "sigmoid",
            inputDim: 2
          })
        );
        model.add(
          tf.layers.dense({
            units: 10,
            useBias: true,
            activation: "sigmoid"
          })
        );
        model.add(
          tf.layers.dense({
            units: 1,
            useBias: true,
            activation: "sigmoid"
          })
        );

        model.compile({
          loss: "binaryCrossentropy",
          optimizer: tf.train.adam()
        });
        return model;
      };

      const prePrepareData = data => {
        // cant be splitted in case there an even size of elements
        if (data.length % 2) {
          data.pop();
        }
        tf.util.shuffle(data);

        return data;
      };

      const posPrepareData = ({ data, featuresTensor, labelsTensor }) => {
        const {
          max: featureMaxValue,
          min: featureMinValue,
          tensor: featuresTensorNormalized
        } = normalize(featuresTensor);

        const {
          max: labelMaxValue,
          min: labelMinValue,
          tensor: labelsTensorNormalized
        } = normalize(labelsTensor);

        const [trainingFeaturesTensor, testingFeaturesTensor] = tf.split(
          featuresTensorNormalized,
          2
        );
        const [trainingLabelsTensor, testingLabelsTensor] = tf.split(
          labelsTensorNormalized,
          2
        );

        return {
          data,
          trainingFeaturesTensor,
          trainingLabelsTensor,
          testingFeaturesTensor,
          testingLabelsTensor,
          labelMinValue,
          labelMaxValue,
          featureMinValue,
          featureMaxValue
        };
      };

      const prepareData = async data => {
        data = prePrepareData(data);

        const values = data.map(p => p.x);

        // second param: data data and number of features (only sqft_living)
        const valuesTensor = tf.tensor2d(values, [values.length, 1]);
        const labelsTensor = tf.tensor2d(labels, [labels.length, 1]);

        return posPrepareData({ data, valuesTensor, labelsTensor });
      };

      const prepareDataWithMultipleFeature = async data => {
        data = prePrepareData(data);

        const features = data.map(d => [d.x, d.y]);
        const featuresTensor = tf.tensor2d(features);

        const labels = data.map(d => d.class);
        const labelsTensor = tf.tensor2d(labels, [labels.length, 1]);

        return posPrepareData({ data, featuresTensor, labelsTensor });
      };

      const trainModel = async (
        model,
        featureTensor,
        labelTensor,
        {
          featureMinValue,
          featureMaxValue,
          labelMinValue,
          labelMaxValue,
          onEpochEnd: onEpoch
        }
      ) => {
        const { onEpochEnd } = tfvis.show.fitCallbacks(
          { name: "Training Performance" },
          ["loss"]
        );

        const result = await model.fit(featureTensor, labelTensor, {
          batchSize: 32,
          epochs: 2,
          validationSplit: 0.2,
          callbacks: {
            onEpochEnd: (epoch, log) => {
              onEpochEnd(epoch, log);
              onEpoch(log);
            }
          }
        });

        await model.save(MODEL_LOAD_PATH);

        return result;
      };

      const predict = (
        model,
        value,
        { featureMinValue, featureMaxValue, labelMinValue, labelMaxValue }
      ) => {
        const tensorPredict = tf.tensor1d([value]);
        const normalizedInput = normalize(
          tensorPredict,
          featureMinValue,
          featureMaxValue
        );

        const normalizedPrediction = model.predict(normalizedInput.tensor);

        const predictionTensor = desnormalize(
          normalizedPrediction,
          labelMinValue,
          labelMaxValue
        );
        return predictionTensor.dataSync()[0];
      };

      async function run() {
        const xLabel = "sqft_living";
        const yLabel = "price";
        const classField = "waterfront";
        const dataset = await loadDataset(xLabel, yLabel, classField);
        const data = await dataset.toArray();
        const model = createModelForBinaryClassification();
        tfvis.show.modelSummary({ name: "model" }, model);

        const {
          data: points,
          trainingFeaturesTensor,
          trainingLabelsTensor,
          testingFeaturesTensor,
          testingLabelsTensor,
          labelMaxValue,
          labelMinValue,
          featureMaxValue,
          featureMinValue
        } = await prepareDataWithMultipleFeature(data);
        console.log({
          points,
          trainingFeaturesTensor,
          trainingLabelsTensor,
          testingFeaturesTensor,
          testingLabelsTensor,
          labelMaxValue,
          labelMinValue,
          featureMaxValue,
          featureMinValue
        });

        visualizePointsAndClasses(model, { data, yLabel, xLabel });

        // const result = await trainModel(
        //   model,
        //   trainingFeaturesTensor,
        //   trainingLabelsTensor,
        //   {
        //     featureMinValue,
        //     featureMaxValue,
        //     labelMinValue,
        //     labelMaxValue,
        //     onEpochEnd: logs => {
        //       console.log(logs);

        //       // visualizePointsAndPredictions(model, {
        //       //   points,
        //       //   featureMinValue,
        //       //   featureMaxValue,
        //       //   labelMinValue,
        //       //   labelMaxValue,
        //       //   xLabel,
        //       //   yLabel
        //       // });
        //     }
        //   }
        // );
        // const trainingLoss = result.history.loss.pop();
        // console.log(`Training set loss: ${trainingLoss}`);

        // const validationLoss = result.history.val_loss.pop();
        // console.log(`Validation set loss: ${validationLoss}`);

        // const lossTensor = model.evaluate(
        //   testingValuesTensor,
        //   testingLabelsTensor
        // );
        // const testingLoss = await lossTensor.dataSync();
        // console.log(`Testing set loss: ${testingLoss}`);

        p = value =>
          predict(model, value, {
            featureMinValue,
            featureMaxValue,
            labelMinValue,
            labelMaxValue
          });
      }

      run();
    </script>
  </body>
</html>
